# RLM-mHC Training Configuration

training:
  # Optimizer (AdamW)
  optimizer:
    type: adamw
    lr: 1e-4
    weight_decay: 0.1
    betas: [0.9, 0.95]
    eps: 1e-8

  # Scheduler (Cosine with warmup)
  scheduler:
    type: cosine
    warmup_steps: 1000
    min_lr_ratio: 0.1

  # Batch configuration
  batch_size: 8
  gradient_accumulation_steps: 4
  max_steps: 50000
  num_epochs: 1

  # Memory optimization
  gradient_checkpointing: true
  mixed_precision: bf16
  max_grad_norm: 1.0

  # Checkpointing
  save_steps: 1000
  save_total_limit: 5
  output_dir: checkpoints

  # Logging
  logging_steps: 10
  wandb:
    project: "rlm-mhc"
    entity: null
    tags: ["1b", "finetune"]

# mHC specific monitoring
mhc:
  monitor_amax: true
  amax_log_steps: 100
  target_amax: 2.0

# Data configuration
data:
  train_path: null  # Set at runtime
  eval_path: null   # Optional
  max_length: 4096
  num_workers: 4
